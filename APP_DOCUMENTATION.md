# YouTube Niche Analysis Dashboard - Complete Technical Documentation

## Table of Contents
1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Setup & Configuration](#setup--configuration)
4. [Core Features](#core-features)
5. [Code Deep Dive](#code-deep-dive)
6. [API Reference](#api-reference)
7. [Troubleshooting](#troubleshooting)

---

## Overview

The YouTube Niche Analysis Dashboard is a Flask-based web application that helps content creators identify high-potential, low-competition niches on YouTube. It uses the YouTube Data API v3 through Replit's connector integration to analyze videos, channels, and engagement metrics.

### Key Capabilities
- **Automated Niche Discovery**: Search for videos by keyword with advanced filters
- **In-App Video Player**: Watch videos directly in the app without leaving
- **Engagement Analysis**: Calculate engagement rates, view velocity, and competition scores
- **Export Functionality**: Download results as CSV for further analysis
- **Quota Management**: Efficient caching system to minimize API quota usage

---

## Architecture

### Technology Stack
```
Backend:  Flask 3.1.2 + Python 3.11
Database: PostgreSQL (via Replit)
Frontend: HTML5 + TailwindCSS + jQuery + DataTables
API:      YouTube Data API v3 (via Replit Connector)
Hosting:  Replit (Gunicorn WSGI server)
```

### Project Structure
```
.
├── app.py                      # Main Flask application
├── main.py                     # Entry point
├── templates/
│   └── index.html             # Frontend UI
├── static/
│   └── style.css              # Custom styles
├── data/
│   ├── cache.json             # API response cache
│   └── search_results.csv     # Latest search results
├── pyproject.toml             # Python dependencies
└── APP_DOCUMENTATION.md       # This file
```

---

## Setup & Configuration

### 1. Environment Setup

The application requires Python 3.11 and the following environment variables:

```python
# Required Environment Variables
SESSION_SECRET          # Flask session secret (auto-generated by Replit)
REPLIT_CONNECTORS_HOSTNAME  # Replit connector API hostname
REPL_IDENTITY          # Replit identity token (development)
WEB_REPL_RENEWAL       # Replit renewal token (production)
```

### 2. YouTube Integration

The app uses **Replit's YouTube Connector** which handles:
- OAuth 2.0 authentication
- Token refresh automatically
- Secure credential storage

#### YouTube API Permissions
The connector has access to these scopes:
- `youtube.readonly` - Read channel and video data
- `youtube` - Full YouTube account access
- `youtubepartner` - Partner-level access
- `yt-analytics.readonly` - Analytics data

### 3. Installing Dependencies

```bash
# Using uv (Replit's package manager)
uv sync

# Dependencies installed:
# - flask==3.1.2
# - gunicorn==23.0.0
# - google-api-python-client==2.184.0
# - google-auth==2.41.1
# - pandas==2.3.3
# - python-dotenv==1.1.1
# - requests==2.32.5
```

### 4. Running the Application

```bash
# Development mode
gunicorn --bind 0.0.0.0:5000 --reuse-port --reload main:app

# Production mode
gunicorn --bind 0.0.0.0:5000 main:app
```

---

## Core Features

### Feature 1: Automated Niche Discovery

**How it works:**
1. User enters keyword (e.g., "cooking tips", "football")
2. App searches YouTube using the `search.list` API endpoint
3. Fetches detailed video metadata using `videos.list`
4. Fetches channel statistics using `channels.list`
5. Applies user-defined filters (subscribers, views, channel age)
6. Calculates custom metrics (potential score, engagement, competition)
7. Returns sorted results with highest potential videos first

**Code Flow:**
```python
# 1. Search videos by keyword
video_ids = automated_search(youtube, keyword, video_duration, max_results)

# 2. Get video details (title, views, likes, duration, etc.)
video_data = get_video_details(youtube, video_ids)

# 3. Get channel details (subscribers, age, video count)
channel_details = get_channel_details(youtube, channel_ids)

# 4. Filter by user criteria
filtered_results = [
    video for video in video_data 
    if min_subs <= channel.subs <= max_subs
    and min_views <= video.views <= max_views
]

# 5. Calculate metrics
results = calculate_metrics(filtered_results, channel_stats)

# 6. Sort by potential score
results = sorted(results, key=lambda x: x['potential_score'], reverse=True)
```

### Feature 2: In-App Video Player

**Implementation:**
- Modal popup with YouTube iframe embed
- Play button in results table
- Video information display (views, likes, channel)
- Keyboard shortcuts (Escape to close)

**HTML Structure:**
```html
<!-- Video Modal -->
<div id="videoModal" class="fixed inset-0 bg-black bg-opacity-75 hidden z-50">
    <div class="bg-white rounded-lg max-w-4xl w-full p-6 relative">
        <h3 id="videoModalTitle"></h3>
        <div class="aspect-video">
            <iframe id="videoPlayer" src="https://www.youtube.com/embed/VIDEO_ID?autoplay=1"></iframe>
        </div>
        <div id="videoInfo"></div>
    </div>
</div>
```

**JavaScript Control:**
```javascript
function openVideoModal(videoId, title, channel, views, likes) {
    $('#videoModal').removeClass('hidden');
    $('#videoPlayer').attr('src', `https://www.youtube.com/embed/${videoId}?autoplay=1`);
    // ... display video info
}

function closeVideoModal() {
    $('#videoModal').addClass('hidden');
    $('#videoPlayer').attr('src', ''); // Stop playback
}
```

### Feature 3: Smart Caching System

**Why caching matters:**
- YouTube API quota: 10,000 units/day
- Each search costs 100 units
- Each video detail fetch costs 1 unit per 50 videos
- Caching saves quota and improves performance

**Implementation:**
```python
# Cache structure
cache = {
    'searches': {
        'search_keyword_duration_maxresults': [video_ids]
    },
    'videos': {
        'video_id': {video_data}
    },
    'channels': {
        'channel_id': {channel_data}
    },
    'channel_details': {
        'channel_details_id': {details}
    }
}

# Caching logic
def automated_search(youtube, keyword, video_duration='short', max_results=20):
    cache = load_cache()
    cache_key = f'search_{keyword}_{video_duration}_{max_results}'
    
    # Check cache first - saves 100 units!
    if cache_key in cache.get('searches', {}):
        print(f"Using cached search results (saved 100 quota units)")
        return cache['searches'][cache_key]
    
    # If not cached, fetch from API and save
    video_ids = fetch_from_youtube(...)
    cache['searches'][cache_key] = video_ids
    save_cache(cache)
    return video_ids
```

### Feature 4: Export to CSV

**Exported columns:**
- Potential Score
- Title, Channel, Video ID
- Channel Age (days), Subscribers
- Views, Likes, Comments
- Duration, Upload Date
- View Velocity, Engagement %
- Competition Score
- Niche keywords

**Usage:**
```python
@app.route('/export')
def export_csv():
    return send_file('data/search_results.csv', 
                     as_attachment=True, 
                     download_name='youtube_analysis.csv')
```

---

## Code Deep Dive

### 1. YouTube Authentication & Token Management

**Problem:** YouTube API tokens expire, causing "credentials do not contain necessary fields" error.

**Solution:** Implement token caching with 5-minute buffer before expiration.

```python
from datetime import datetime, timedelta, timezone
from typing import Any, Optional

connection_settings_cache: dict[str, Any] = {'data': None, 'expires_at': None}

def get_access_token(force_refresh=False):
    """Get fresh access token with intelligent caching
    
    Args:
        force_refresh: If True, bypass cache and get fresh token
    
    Returns:
        str: Valid YouTube API access token
    """
    global connection_settings_cache
    
    # Add 5 minute buffer before expiration to avoid edge cases
    buffer_time = timedelta(minutes=5)
    
    if (not force_refresh and 
        connection_settings_cache['data'] and 
        connection_settings_cache['expires_at'] and 
        datetime.now(timezone.utc) < (connection_settings_cache['expires_at'] - buffer_time)):
        print("Using cached access token")
        return connection_settings_cache['data'].get('settings', {}).get('access_token')
    
    print("Fetching fresh access token from connector")
    connection_settings = get_youtube_connection_info()
    
    # Parse expiration time
    expires_at_str = settings.get('expires_at')
    if expires_at_str:
        expires_at = datetime.fromisoformat(expires_at_str.replace('Z', '+00:00'))
        connection_settings_cache['expires_at'] = expires_at
    else:
        # Default to 30 minutes if no expiration provided
        connection_settings_cache['expires_at'] = datetime.now(timezone.utc) + timedelta(minutes=30)
    
    connection_settings_cache['data'] = connection_settings
    return access_token
```

### 2. Replit Connector Integration

**How it works:**
```python
def get_youtube_connection_info():
    """Fetch YouTube connection from Replit Connector API
    
    Returns:
        dict: Connection settings including access_token, expires_at
    """
    hostname = os.getenv('REPLIT_CONNECTORS_HOSTNAME')
    
    # Build authentication headers
    repl_identity = os.getenv('REPL_IDENTITY')
    web_repl_renewal = os.getenv('WEB_REPL_RENEWAL')
    
    x_replit_token = None
    if repl_identity:
        x_replit_token = 'repl ' + repl_identity  # Development
    elif web_repl_renewal:
        x_replit_token = 'depl ' + web_repl_renewal  # Production
    
    # Fetch connection settings
    response = requests.get(
        f'https://{hostname}/api/v2/connection?include_secrets=true&connector_names=youtube',
        headers={
            'Accept': 'application/json',
            'X_REPLIT_TOKEN': x_replit_token
        }
    )
    
    connection_data = response.json()
    return connection_data['items'][0]
```

### 3. Metric Calculations

**Potential Score** - Identifies channels with high views but low subscribers (best opportunities):

```python
def calculate_potential_score(video_data, channel_data):
    """Calculate potential score for videos (high views, low subs = high potential)
    
    Formula:
    - Base score = (views / subscribers) * 100
    - Bonus for small channels (<10k subs): 1.5x multiplier
    - Bonus for medium channels (<50k subs): 1.2x multiplier
    - Capped at 100
    
    Args:
        video_data: Dict with 'views' key
        channel_data: Dict with 'subscriber_count' key
    
    Returns:
        float: Potential score (0-100)
    """
    views = video_data.get('views', 0)
    subs = channel_data.get('subscriber_count', 1)
    
    if subs == 0:
        subs = 1  # Avoid division by zero
    
    view_to_sub_ratio = views / subs
    base_score = min(view_to_sub_ratio * 100, 100)
    
    # Apply channel size multipliers
    if subs < 10000:
        base_score *= 1.5  # Small channels get 50% bonus
    elif subs < 50000:
        base_score *= 1.2  # Medium channels get 20% bonus
    
    return min(base_score, 100)
```

**Engagement Rate** - Measures audience interaction:

```python
def calculate_engagement(video):
    """Calculate engagement percentage
    
    Formula: ((likes + comments) / views) * 100
    
    Returns:
        float: Engagement percentage
    """
    if video['views'] == 0:
        return 0
    
    engagement = ((video['likes'] + video['comments']) / video['views']) * 100
    return round(engagement, 2)
```

**Competition Score** - Estimates difficulty of ranking in niche:

```python
def calculate_competition(channel_stats, video_stats):
    """Calculate competition score (0-100, lower is better)
    
    Factors:
    - Channel subscribers (40% weight)
    - Channel video count (30% weight)
    - View velocity (30% weight)
    
    Returns:
        float: Competition score (0-100)
    """
    subs = channel_stats.get('subscriber_count', 0)
    video_count = channel_stats.get('video_count', 0)
    view_velocity = video_stats.get('view_velocity', 0)
    
    competition = (
        (subs / 1000000 * 40) +          # Subscriber factor
        (video_count / 1000 * 30) +      # Content saturation
        (view_velocity / 10000 * 30)     # Velocity factor
    )
    
    return min(competition, 100)
```

**View Velocity** - Measures viral potential:

```python
def calculate_view_velocity(video):
    """Calculate views per day since upload
    
    Returns:
        float: Average views per day
    """
    upload_date = datetime.fromisoformat(video['upload_date'].replace('Z', '+00:00'))
    days_since_upload = (datetime.now(upload_date.tzinfo) - upload_date).days
    days_since_upload = max(days_since_upload, 1)  # Minimum 1 day
    
    view_velocity = video['views'] / days_since_upload
    return round(view_velocity, 2)
```

### 4. Efficient YouTube API Batching

**Video Details Batching** (1 unit per 50 videos):

```python
def get_video_details(youtube, video_ids):
    """Fetch video details with efficient batching
    
    Cost: 1 quota unit per batch of 50 videos
    
    Args:
        youtube: Authenticated YouTube client
        video_ids: List of video IDs to fetch
    
    Returns:
        list: Video details
    """
    cache = load_cache()
    results = []
    uncached_ids = []
    
    # Check cache first
    for video_id in video_ids:
        if video_id in cache.get('videos', {}):
            results.append(cache['videos'][video_id])
        else:
            uncached_ids.append(video_id)
    
    # Batch fetch uncached videos (50 per request)
    if uncached_ids:
        for i in range(0, len(uncached_ids), 50):
            batch = uncached_ids[i:i+50]
            print(f"Fetching details for {len(batch)} videos (1 quota unit)")
            
            response = youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=','.join(batch)  # Comma-separated IDs
            ).execute()
            
            # Process and cache results
            for item in response.get('items', []):
                video_data = {
                    'video_id': item['id'],
                    'title': item['snippet']['title'],
                    'views': int(item['statistics'].get('viewCount', 0)),
                    'likes': int(item['statistics'].get('likeCount', 0)),
                    # ... more fields
                }
                results.append(video_data)
                
                if 'videos' not in cache:
                    cache['videos'] = {}
                cache['videos'][item['id']] = video_data
        
        save_cache(cache)
    
    return results
```

### 5. Replit Proxy Configuration

**Problem:** App generates URLs with `http://0.0.0.0:5000` which browsers can't access.

**Solution:** Configure ProxyFix middleware to use Replit's HTTPS proxy.

```python
from flask import Flask
from werkzeug.middleware.proxy_fix import ProxyFix

app = Flask(__name__)
app.config['SECRET_KEY'] = os.getenv('SESSION_SECRET')
app.config['PREFERRED_URL_SCHEME'] = 'https'  # Force HTTPS URLs

# ProxyFix middleware handles X-Forwarded-* headers
app.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)
```

**What this does:**
- `x_proto=1` - Trust X-Forwarded-Proto header (http/https)
- `x_host=1` - Trust X-Forwarded-Host header (replit.dev domain)
- `PREFERRED_URL_SCHEME='https'` - Generate https:// URLs by default
- Result: `url_for()` generates proper Replit URLs instead of localhost

---

## API Reference

### YouTube Data API v3 Endpoints Used

#### 1. `search.list` - Search for videos
**Cost:** 100 units per request
```python
youtube.search().list(
    part='id',
    q=keyword,                    # Search query
    type='video',                 # Only videos
    videoDuration=duration,       # short/medium/long/any
    maxResults=20,                # Results per page (max 50)
    order='viewCount'             # Sort by views
).execute()
```

#### 2. `videos.list` - Get video details
**Cost:** 1 unit per request (up to 50 videos)
```python
youtube.videos().list(
    part='snippet,statistics,contentDetails',
    id='video_id1,video_id2,...'  # Comma-separated IDs
).execute()
```

#### 3. `channels.list` - Get channel statistics
**Cost:** 1 unit per request (up to 50 channels)
```python
youtube.channels().list(
    part='snippet,statistics',
    id='channel_id1,channel_id2,...'
).execute()
```

### Flask API Routes

#### `POST /search` - Automated niche search
```javascript
// Request
{
  "keyword": "cooking tips",
  "video_duration": "short",
  "min_subs": 0,
  "max_subs": 999999999,
  "min_views": 0,
  "max_views": 999999999,
  "max_channel_age_days": null,
  "max_results": 20
}

// Response
{
  "success": true,
  "data": [
    {
      "video_id": "abc123",
      "title": "10 Cooking Tips...",
      "channel": "Chef John",
      "channel_id": "UC...",
      "views": 50000,
      "likes": 2000,
      "potential_score": 85.5,
      "engagement_pct": 4.2,
      "competition_score": 35.8
    }
  ],
  "count": 20,
  "search_params": {...}
}
```

#### `GET /export` - Download CSV
```javascript
// Returns: CSV file download
// Filename: youtube_analysis.csv
```

#### `GET /api/account/info` - Get YouTube account
```javascript
// Response
{
  "success": true,
  "account": {
    "name": "DARKLIGHT ANIMATIONS",
    "email": "user@gmail.com",
    "subscribers": 0
  }
}
```

---

## Troubleshooting

### Issue 1: "credentials do not contain necessary fields" Error

**Cause:** Access token expired and YouTube SDK tried to refresh but lacked refresh_token.

**Solution:** Implemented 5-minute buffer before expiration + force refresh option.

```python
# The fix
buffer_time = timedelta(minutes=5)
if datetime.now(timezone.utc) < (expires_at - buffer_time):
    # Use cached token (still valid)
else:
    # Fetch fresh token (about to expire)
```

### Issue 2: "No videos match your filter criteria"

**Cause:** Default max_subs was 100,000, filtering out most results.

**Solution:** Changed default to 999,999,999 (no practical limit).

```html
<!-- Before -->
<input id="maxSubs" value="100000">

<!-- After -->
<input id="maxSubs" value="999999999">
```

### Issue 3: "Hmm... We couldn't reach this app"

**Cause:** App generating `http://0.0.0.0:5000` URLs that browsers can't access.

**Solution:** Added ProxyFix middleware to use Replit's HTTPS proxy.

```python
app.config['PREFERRED_URL_SCHEME'] = 'https'
app.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)
```

### Issue 4: Quota Exceeded

**Symptoms:**
```json
{
  "error": "YouTube API Quota Exceeded",
  "details": "Your YouTube API quota has been exceeded..."
}
```

**Solutions:**
1. Wait until next day (quota resets at midnight Pacific Time)
2. Use cached results (searches show "saved 100 quota units")
3. Reduce max_results to conserve quota
4. Increase quota in Google Cloud Console

### Issue 5: Video Player Not Loading

**Possible causes:**
- YouTube video is private/deleted
- Embedding disabled by uploader
- Network connectivity issues

**Debug:**
```javascript
// Check browser console for errors
console.log('Video ID:', videoId);
console.log('Iframe src:', $('#videoPlayer').attr('src'));
```

---

## Performance Optimization Tips

### 1. Quota Conservation
```python
# Lower max_results values
max_results = 10  # Instead of 100

# Check cache before searching
cache_key = f'search_{keyword}_{duration}_{max_results}'
if cache_key in cache:
    return cached_results  # Saves 100 units!
```

### 2. Efficient Batching
```python
# Bad: 50 separate requests (50 units)
for video_id in video_ids:
    youtube.videos().list(id=video_id).execute()

# Good: 1 request (1 unit)
youtube.videos().list(id=','.join(video_ids[:50])).execute()
```

### 3. Database Queries
```python
# Use bulk operations
channel_ids = list(set([v['channel_id'] for v in videos]))
channel_details = get_channel_details(youtube, channel_ids)
```

---

## Future Enhancements

### Planned Features
1. **Trending Analysis** - Track niche growth over time
2. **Competitor Tracking** - Monitor specific channels
3. **Content Calendar** - Schedule content based on trends
4. **AI Suggestions** - GPT-powered title/thumbnail recommendations
5. **Multi-Language Support** - Analyze niches in different languages

### Code Improvements
1. **Database Storage** - Store search history in PostgreSQL
2. **User Accounts** - Save favorite niches per user
3. **Background Jobs** - Async processing for large searches
4. **Real-time Updates** - WebSocket for live quota tracking
5. **Advanced Filtering** - Duration ranges, upload dates, etc.

---

## License & Credits

**Built with:**
- Flask (BSD License)
- YouTube Data API v3 (Google)
- TailwindCSS (MIT License)
- DataTables (MIT License)
- Replit Connectors

**Author:** Built for niche analysis and content strategy

**Support:** For issues, check logs at `/tmp/logs/` or contact via Replit

---

## Quick Reference

### Common Commands
```bash
# Install dependencies
uv sync

# Run development server
gunicorn --bind 0.0.0.0:5000 --reuse-port --reload main:app

# Check logs
tail -f /tmp/logs/Start_application_*.log

# Clear cache
rm data/cache.json

# Export results
curl http://localhost:5000/export > results.csv
```

### Important Files
- `app.py` - Main application logic
- `templates/index.html` - Frontend UI
- `data/cache.json` - API response cache
- `data/search_results.csv` - Latest search export
- `.replit` - Replit configuration

### Key Environment Variables
- `SESSION_SECRET` - Flask session key
- `REPLIT_CONNECTORS_HOSTNAME` - Connector API host
- `REPL_IDENTITY` / `WEB_REPL_RENEWAL` - Auth tokens

---

**Last Updated:** November 18, 2025
**Version:** 1.0.0
